# -*- coding: utf-8 -*-
"""gpt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YlTBEI-gZre51DC84wK75NTkGoAbiay2
"""

"""
Minimal decoder-only GPT implementation for WIDS Final Project
Save this file as gpt.py and run: python gpt.py

Requirements satisfied (look for these class/comments in file):
- Head (One head of self-attention)
- MultiHeadAttention (Running multiple heads in parallel)
- FeedForward (Linear layers with non-linearity)
- Block (Combining Attention and FeedForward)
- Positional Embeddings (So the model knows the order of words)

Notes:
- Expects a text file called `input.txt` in the same directory.
- Default training iterations = 4000 (change in config below).
- Prints/prints generated sample text and final validation loss.

This implementation is intentionally small and readable (inspired by minGPT) so it runs on modest hardware.
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
import math
import os
import time

# ------------------ configuration ------------------
class Config:
    block_size = 128    # context length
    batch_size = 64
    max_iters = 4000    # default training iterations; change as needed
    eval_interval = 200
    learning_rate = 3e-4
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    n_embd = 192
    n_head = 6
    n_layer = 6
    dropout = 0.1
    # generation params
    gen_tokens = 400
    temperature = 1.0
    top_k = 65C = Config()

# ------------------ data loading ------------------
if not os.path.exists('input.txt'):
    raise SystemExit("Place a training file named 'input.txt' in the same folder as gpt.py and re-run.")

with open('input.txt', 'r', encoding='utf-8') as f:
    data = f.read()

# build vocabulary
chars = sorted(list(set(data)))
vocab_size = len(chars)
print(f"data has {len(data)} characters, {vocab_size} unique")

stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

def encode(s):
    return [stoi[c] for c in s]

def decode(l):
    return ''.join([itos[i] for i in l])

# train/val split
n = len(data)
train_data = data[:int(n*0.9)]
val_data = data[int(n*0.9):]

train_ids = torch.tensor(encode(train_data), dtype=torch.long)
val_ids = torch.tensor(encode(val_data), dtype=torch.long)

# helper to fetch batches
def get_batch(split):
    data_src = train_ids if split == 'train' else val_ids
    ix = torch.randint(len(data_src) - C.block_size, (C.batch_size,))
    x = torch.stack([data_src[i:i+C.block_size] for i in ix])
    y = torch.stack([data_src[i+1:i+C.block_size+1] for i in ix])
    return x.to(C.device), y.to(C.device)

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train','val']:
        losses = torch.zeros(0).to(C.device)
        iters = 10
        for _ in range(iters):
            xb,yb = get_batch(split)
            logits, loss = model(xb, yb)
            losses = torch.cat((losses, loss.reshape(1)))
        out[split] = losses.mean().item()
    model.train()
    return out

# ------------------ model components ------------------

# Head (one attention head)
class Head(nn.Module):
    """One head of self-attention"""
    def __init__(self, embed_size, head_size, dropout):
        super().__init__()
        self.key = nn.Linear(embed_size, head_size, bias=False)
        self.query = nn.Linear(embed_size, head_size, bias=False)
        self.value = nn.Linear(embed_size, head_size, bias=False)
        self.dropout = nn.Dropout(dropout)
        # causal mask will be registered later as buffer
        self.register_buffer('tril', torch.tril(torch.ones(C.block_size, C.block_size)))

    def forward(self, x):
        B, T, Cc = x.size()
        k = self.key(x)   # (B,T,hs)
        q = self.query(x) # (B,T,hs)
        # compute attention scores
        att = q @ k.transpose(-2,-1) * (1.0 / math.sqrt(k.size(-1)))
        # causal mask: (T,T) lower-triangular
        mask = self.tril[:T, :T]
        att = att.masked_fill(mask == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        att = self.dropout(att)
        v = self.value(x)
        out = att @ v  # (B,T,hs)
        return out

# MultiHeadAttention (parallel heads)
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, num_heads, dropout):
        super().__init__()
        assert embed_size % num_heads == 0
        self.head_size = embed_size // num_heads
        self.num_heads = num_heads
        self.heads = nn.ModuleList([Head(embed_size, self.head_size, dropout) for _ in range(num_heads)])
        self.proj = nn.Linear(embed_size, embed_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (B, T, embed)
        out = torch.cat([h(x) for h in self.heads], dim=-1)  # (B,T,embed)
        out = self.dropout(self.proj(out))
        return out

# FeedForward (simple MLP)
class FeedForward(nn.Module):
    def __init__(self, embed_size, hidden_mult=4, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(embed_size, hidden_mult * embed_size),
            nn.GELU(),
            nn.Linear(hidden_mult * embed_size, embed_size),
            nn.Dropout(dropout),
        )
    def forward(self, x):
        return self.net(x)

# Block (combines attention and feedforward with residuals)
class Block(nn.Module):
    def __init__(self, embed_size, num_heads, dropout):
        super().__init__()
        self.attn = MultiHeadAttention(embed_size, num_heads, dropout)
        self.ffwd = FeedForward(embed_size, hidden_mult=4, dropout=dropout)
        self.ln1 = nn.LayerNorm(embed_size)
        self.ln2 = nn.LayerNorm(embed_size)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# ------------------ full GPT language model ------------------
class GPTLanguageModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, C.n_embd)
        # Positional Embeddings
        self.position_embedding_table = nn.Embedding(C.block_size, C.n_embd)
        self.blocks = nn.Sequential(*[Block(C.n_embd, C.n_head, C.dropout) for _ in range(C.n_layer)])
        self.ln_f = nn.LayerNorm(C.n_embd)
        self.head = nn.Linear(C.n_embd, vocab_size, bias=False)

        self.drop = nn.Dropout(C.dropout)

        # initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_emb = self.token_embedding_table(idx) # (B,T,emb)
        pos = torch.arange(T, device=idx.device).unsqueeze(0)
        pos_emb = self.position_embedding_table(pos) # (1,T,emb)
        x = self.drop(token_emb + pos_emb)
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.head(x) # (B,T,vocab)

        if targets is None:
            return logits, None
        # compute loss
        B, T, V = logits.shape
        logits = logits.view(B*T, V)
        targets = targets.view(B*T)
        loss = F.cross_entropy(logits, targets)
        return logits.view(B,T,V), loss

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """Generate from the model given initial context idx (1D tensor of indices)
        Returns generated indices (including the context)
        """
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -C.block_size:]
            logits, _ = self(idx_cond)
            # focus only on last time step
            logits = logits[:, -1, :] / (temperature if temperature>0 else 1.0)
            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                minv = v[:, -1].unsqueeze(1)
                logits = torch.where(logits < minv, torch.full_like(logits, -1e10), logits)
            probs = F.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, next_id), dim=1)
        return idx

# instantiate model
model = GPTLanguageModel().to(C.device)

# optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=C.learning_rate)

# ------------------ training loop ------------------

def train():
    best_val_loss = float('inf')
    t0 = time.time()
    for it in range(C.max_iters):
        if it % C.eval_interval == 0:
            losses = estimate_loss()
            print(f"step {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")
            best_val_loss = min(best_val_loss, losses['val'])
        xb, yb = get_batch('train')
        logits, loss = model(xb, yb)
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

    total_time = time.time() - t0
    print(f"Training completed in {total_time:.1f}s")
    # final evaluation
    final_losses = estimate_loss()
    print(f"Final: train loss {final_losses['train']:.4f}, val loss {final_losses['val']:.4f}")

    # generate sample
    context = torch.zeros((1,1), dtype=torch.long, device=C.device)
    generated = model.generate(context, max_new_tokens=C.gen_tokens, temperature=C.temperature, top_k=C.top_k)
    text = decode(generated[0].tolist())
    print('\n' + '='*40 + '\nSAMPLE GENERATED TEXT:\n' + '='*40 + '\n')
    print(text)
    print('\n' + '='*40)

if __name__ == '__main__':
    print('Configuration:')
    for k,v in C.__dict__.items():
        if not k.startswith('__') and not callable(v):
            print(f"  {k}: {v}")
    print('\nStarting training...')
    train()
    # save final model weights
    torch.save(model.state_dict(), 'gpt_final.pt')
    print('Model saved to gpt_final.pt')